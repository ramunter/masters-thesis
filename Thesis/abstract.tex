\clearpage
\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\section*{\Huge Summary}
\addcontentsline{toc}{chapter}{Summary}	
$\\[0.5cm]$

\noindent Using bayesian methods in reinforcement learning can lead to near optimal results on the exploration-exploitation trade-off. However for these methods to be successful one should take into account the relationship between different different states. Recent papers have shown that by propagating variance between states it is possible to improve on current exploration methods. This thesis empirically shows that it is possible to implicitly propagate variance between states with a linear conjugate prior regression model. Furthermore the method is extended to neural networks showing it is possible to use this variance propagation method in more complex environments. However in it's current form the propagated variance is dependent only on the action, further research is required to extend this to a state-dependent variance.

 

\clearpage