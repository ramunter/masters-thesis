%===================================== CHAP 5 =================================
\chapter{BDQN}{\label{ch:ube}}

\section{Linear bayesian Q learning}

\subsection{Linear Q learning}

In linear Q learning the goal is to create a regression model that maps the state and action to a Q-value, $Q(s,a)$. Let $x_t$ denote the state and action at timestep $t$. $X$ then denotes the design matrix containing these features and $Q$ the vector of corresponding Q-values. The regression model can then be defined as

\begin{equation*}
	Q = X\beta + \varepsilon \quad \text{where} \quad \varepsilon \sim N(0,\sigma^2)
\end{equation*}

with the response value defined as 

\begin{equation}
    \label{eq:q_target}
	Q(s,a) = r_t + \argmax_{a'}Q(s',a').
\end{equation}

The ordinary least squares solution to the $\beta$ coefficients can then be found using the normal equation which in matrix form is

\begin{equation*}
	\beta = [X^TX]^{-1}X^TQ
\end{equation*}

\subsection{Linear Bayesian Q learning}

Since thompson sampling is known to give the exploration in a bandit setting it is of interest to view the regression from a bayesian setting where all parameters are viewed as random variables. Following the notation from linear regression the target Q-value is then defined by the following likelihood function

\begin{equation*}
    p(\mathbf {Q} |\mathbf {X} ,{\boldsymbol {\beta }},\sigma ^{2})\propto (\sigma ^{2})^{-{\frac {v}{2}}}\exp \left(-{\frac {vs^{2}}{2{\sigma }^{2}}}\right)(\sigma ^{2})^{-{\frac {n-v}{2}}}\exp \left(-{\frac {1}{2{\sigma }^{2}}}({\boldsymbol {\beta }}-{\hat {\boldsymbol {\beta }}})^{\rm {T}}(\mathbf {X} ^{\rm {T}}\mathbf {X} )({\boldsymbol {\beta }}-{\hat {\boldsymbol {\beta }}})\right)
\end{equation*}

$X$, the design matrix, is given by the agents experiences. However both $\beta$ and $\sigma^2$ needs a prior distribution. By definition $\varepsilon$ is conditionally independent of $X\beta$ leaving $p(\beta, \sigma^2) = p(\beta|\sigma^2)p(\sigma^2)$.

To simplify calculations and implementation we want to select conjugate priors. In literature it is common to use a gaussian distribution over $\beta$ and an inverse-gamma distribution over $\sigma^2$.

\begin{align*}
	  & p(\sigma^2) = \text{InvGamma}(\alpha, b)          \\
	  & p(\beta|\sigma^2) = \text{N}(\mu, \sigma^2\Sigma) 
\end{align*}

Note that this prior assumption is purely based on its conjugate priors, not any aspects of regression or reinforcement learning task.

\todo I'm going to skip how to derive the posterior for now

The posterior is then defined as

\begin{align*}
	\Lambda_n & = (X^TX + \Lambda_0)                                         \\
	\mu_n     & = \Lambda_n^{-1}(\Lambda_0\mu_0 + X^TQ)                      \\
	\alpha_n  & = \alpha_0 + \frac{n}{2}                                     \\
	b_n       & = b_0 + (Q^TQ + \mu^T\Lambda_0\mu_0 - \mu_n^T\Lambda_n\mu_n) 
\end{align*}

Finally the choice of $\beta$ in the target(eq \ref{eq:q_target}) remains. In \cite{azziz_2018} the MAP estimate of $\beta$ is used. For the above priors this is $\mu_n$, the mean $\beta$ value. However this method ignores the variance in the target and so does not propagate the uncertainty between states. To incorporate this uncertainty one can also sample the $\beta$ used for target calculation. In this way the bayesian regression model must take into account the variance in the target.

\section{Bayesian Deep Q Network}

\todo this section assumes DQN has been explained

The predominant issue with bayesian methods in deep reinforcement learning is using bayesian methods with neural networks. This thesis will address the linear layer method (\todo Actual name for method), a simple and computationally efficient method that comes at the cost of accuracy. 

The final layer in a DQN is a linear layer. Since bayesian regression is also a linear combination one can replace the final layer with a bayesian regression model per action. This is equivalent to rewriting the regression task to 

\begin{equation*}
	Q = \phi(X)\beta + \varepsilon \quad \text{where} \quad \varepsilon \sim N(0,\sigma^2)
\end{equation*}

where $\phi(X)$ is the neural networks output given an input $X$. Note that this means the bayesian regression no longer incorporates all the uncertainty since the above assumes no uncertainty in the $\phi(X)$ encoding. 

Training the model now needs to be split into two processes. Firstly the bayesian regression is trained using the posterior update shown above. The neural network is trained using a similar loss function as the DQN. However the networks Q-value estimate is replaced by the MAP estimate of $\beta$ resulting in

\begin{equation*}
	\theta = \theta - \alpha\nabla_\theta\big(Q_t - [\mu_n^T\phi_\theta(x_t)]\big)^2.
\end{equation*}

Note that these do not have to happen sequentially. In \cite{azziz_2018} and this implementation the bayesian regression is updated less often than the neural network.

Finally to deal with the fact that reinforcement learning is a non-stationary problem the the bayesian regression is trained for scratch each time it is updated.

\cleardoublepage