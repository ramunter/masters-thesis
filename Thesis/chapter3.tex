%===================================== CHAP 5 =================================
\chapter{Bayesian Q Learning}{\label{ch:bdqn}}

In an attempt to find a better balance between exploration and exploitation this thesis investigates the use of bayesian methods to allow for Thompson sampling. This chapter builds and compares bayesian methods in a linear model context before attempting to extend the most successful methods to neural networks.

\section{Linear Q learning}

In linear Q learning the goal is to create a regression model that maps the state and action to a Q-value, $Q(s,a)$. Let $x_t$ denote the state and action at timestep $t$. $X$ then denotes the design matrix containing these features and $Q$ the vector of corresponding Q-values. The regression model for a single action can then be defined as

\begin{equation*}
	Q(X,a) = X\beta + \varepsilon \quad \text{where} \quad \varepsilon \sim N(0,\sigma^2)
\end{equation*}

with the response value defined as 

\begin{equation}
    \label{eq:q_target}
	Q(s,a) = r_t + \argmax_{a'}Q(s',a').
\end{equation}

The ordinary least squares solution to the $\beta$ coefficients can then be found using the normal equation which in matrix form is

\begin{equation*}
	\beta = [X^TX]^{-1}X^TQ
\end{equation*}

Given this model the agent can take an action by acting greedily over the models $Q(s,a)$ values in a given state. Since this purely an exploitation strategy, it is often coupled with the $\varepsilon$-greedy policy.

\section{Bayesian Linear Q learning}

To extend linear Q learning methods to follow a Thompson sampling policy a bayesian perspective is required. In a RL perspective the goal is to model the posterior

\begin{align*}
p(\theta |Q, \mathcal{F}) &\propto p(Q| \theta, \mathcal{F})p(\theta) \\
p(Q) &= \int p(Q|\theta, \mathcal{F}), p(\theta) d\theta
\end{align*}

where $Q$ is a vector of all Q-values given the state $X_t$, $\theta$ denotes all parameters and $\mathcal{F}$ denotes all previous transitions. In RL the value of interest are samples from $p(Q|\theta, \mathcal{F})$, not it's actual value.

The calculation of an arbitrary posterior is computationally heavy which is ill-suited to the already long running reinforcement learning methods. In order to keep computation costs low to start off this thesis will consider conjugate priors which have an analytical solution.

\subsection{Normal Prior with Known noise}

There are multiple ways to setup a bayesian regression model using conjugate priors. First consider the case used in \cite{azziz_2018} which creates one model per action and assumes the noise variance is known. The known noise variance is then treated as a hyperparameter. In this case the posterior can be expressed as 

\begin{align*}
    p(\beta_a |Q_a, \sigma_{\varepsilon_a}, \mathcal{F}) &\propto p(Q_a| \beta_a, \sigma_{\varepsilon_a}, \mathcal{F})p(\beta_a) \\
    p(Q_a|\sigma_{\varepsilon_a}, \mathcal{F}) &= \int p(Q_a|\beta_a, \sigma_{\varepsilon_a}, \mathcal{F}) p(\beta_a)d\beta_a
\end{align*}

In literature it is common to use a gaussian prior for $\beta$

$$
p(\beta) = \text{N}(\mu, \sigma_\varepsilon\Lambda^{-1}) 
$$

where $\Lambda$ is the precision matrix. This results in the following posterior update

\begin{align*}
	\Lambda_n & = X^TX + \Lambda_0 \\
	\mu_n     & = \Lambda_n^{-1}(\Lambda_0\mu_0 + X^TQ_a)
\end{align*}

Note that when picking actions the sampled $Q$-value is used. However when calculating the target $Q$-value the MAP estimate of $\beta$ is used instead. In this case the MAP estimate is $\mu$. 

\subsection{Normal Prior with Unknown noise}

To avoid the noise variance as a hyperparameter it can be included as an unknown parameter.

\begin{align*}
    p(\beta_a, \sigma_{\varepsilon_a}|Q_a, \mathcal{F}) &\propto p(Q_a| \beta_a, \sigma_{\varepsilon_a}, \mathcal{F})p(\theta) \\
    p(Q_a|\mathcal{F}) &= \int p(Q_a|\beta_a, \sigma_{\varepsilon_a}, \mathcal{F}) p(\beta_a, \sigma_{\varepsilon_a})d\beta_a d\sigma_{\varepsilon_a}
\end{align*}

The conjugate priors for this setup are

\begin{align*}
    & p(\sigma^2) = \text{InvGamma}(\alpha, b)          \\
    & p(\beta|\sigma^2) = \text{N}(\mu, \sigma^2\Sigma) 
\end{align*}

with the posterior update

\begin{align*}
	\Lambda_n & = (X^TX + \Lambda_0)                                         \\
	\mu_n     & = \Lambda_n^{-1}(\Lambda_0\mu_0 + X^TQ)                      \\
	\alpha_n  & = \alpha_0 + \frac{n}{2}                                     \\
	b_n       & = b_0 + (Q^TQ + \mu^T\Lambda_0\mu_0 - \mu_n^T\Lambda_n\mu_n) 
\end{align*}

Once again $\mu$, the MAP estimate of $\beta$ is used to calculate the target Q-value.

\todo: These results are outdated

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results_no_prop.png}
    \caption{WIP: \textbf{Performance on chain environment} Guassian Prior 2 includes the noise as a parameter}
    \label{fig:results_no_propr}
\end{figure}


\subsection{Propagating Uncertainty}

One possible issue with the two above methods is training using the MAP estimate of $\beta$. Using the MAP estimate means that the targets come from the following process:

$$
y = R + \max_a X_{t+1}\mu_a.
$$

Though this does incorporate the variance in the reward process through $R$ it does not convey the variance in the Q-value estimate of the next state. Even in a deterministic environment the policy shifts during training mean that there is an uncertainty in the Q-value of the next state. Quoting \cite{moerland_2017},  "\dots repeatedly visiting a state-action pair should not makes us certain about its value if we are still uncertain about what to do next."

One possible method to include this uncertainty is to sample the $\beta$ posterior when calculating the target value.

\todo Here is where I want stronger argumentation of why this should work (other than intuition).

\todo These results are outdated

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{results_with_prop.png}
    \caption{WIP: \textbf{Performance on chain environment} Guassian Prior 2 includes the noise as a parameter}
    \label{fig:results_no_propr}
\end{figure}

\subsection{Investigating propagation}

To ensure that these methods are propagating uncertainty we can consider a simple example from \cite{osband_2018}. Consider a MPD with two states. The initial state allows only one action that deterministicly leads to state 2 with no reward. State 2 is a terminal state with a known posterior distribution.

If a RL method properly propagates uncertainty the posterior distribution of state 1 should match state 2 as long as $\gamma=1$. To test the following setup was usedThe priors for the known noise models were set to

\begin{align*}
    \beta &\sim N(0,10^3)\\
    \sigma^2 &= 1
\end{align*}

and the priors for the unknown noise models were set to

\begin{align*}
    \beta &\sim N(0,10^3) \\
    \sigma^2 &\sim InvGamma(1,1).
\end{align*}

Three MDP's were set up with a known posterior of $N(1, 0.1), N(1, 1)$ and $N(1, 10)$ respectively. The results are seen in figure \ref{fig:proptest}.

\begin{figure}[H]
    \centering
    \subfloat[$N(1,0.1^2)$]{
        \includegraphics[width=0.8\textwidth]{1000iterationsSD01.png}
    }
    \\
    \subfloat[$N(1,1^2)$]{
        \includegraphics[width=0.8\textwidth]{1000iterationsSD1.png}
    }
    \\
    \subfloat[$N(1,10^2)$]{
        \includegraphics[width=0.8\textwidth]{1000iterationsSD10.png}
    }
    \caption{\textbf{Variance Propagation On 2 State Toy Example}: It is clear that the known noise models result in a normal distribution around the correct mean but with the assumed noise level for variance. The unknown noise model using the mean target leads to a variance that heads towards 0. The only model that is able to correctly model the posterior is the unknown noise model with sampled targets.}
    \label{fig:proptest}
\end{figure}

Based on these results focus is placed on the unknown noise model with sampled targets. Now consider a modification to the environment where an extra state is placed between the initial and terminal state. This state has the same dynamics meaning it deterministic transitions to the terminal state with zero reward over the transition. This allows an investigation over how well variance is propagated through multiple states.

\begin{figure}[H]
    \centering

    \subfloat[$N(1,1^2)$]{
        \includegraphics[width=0.5\textwidth]{3State10000iterationsSD1.png}
    }
    \subfloat[$N(1,10^2)$]{
        \includegraphics[width=0.5\textwidth]{3State10000iterationsSD10.png}
    }
    \\
    \subfloat[$N(1,0.1^2)$]{
        \includegraphics[width=0.5\textwidth]{3State10000iterationsSD01.png}
    }
    \subfloat[$N(1,0.01^2)$]{
        \includegraphics[width=0.5\textwidth]{3State10000iterationsSD001.png}
    }
    \caption{\textbf{Variance Propagation On 3 State Toy Example}: For larger variance targets the propagation correctly updates the state 1 variance. However the low variance targets results in an overestimation over the variance which worsens further away from the known posterior.}
    \label{fig:3stateproptest}
\end{figure}

One reason the 0.01 standard deviation posterior is harder to approximate than the 1 sd is that the sensitivity of the posterior to the $\alpha$ and $\beta$ parameters. Small changes in these for a distribution with low variance leads to large changes (\todo: Confirm that this is actually the case)


\begin{figure}[H]
    \centering
    \subfloat[6 States with $N(0.01,1^2)$]{
        \includegraphics[width=0.8\textwidth]{6State10000iterationsSD001.png}
    }
    \\
    \subfloat[11 States with $N(1,1^2)$ target]{
        \includegraphics[width=0.8\textwidth]{11State10000iterationsSD1.png}
    }

    \caption{\textbf{Failure of variance propagation over many states}: (\textbf{a}) shows that the error in estimation close to the terminal state leads to failure in the estimation of the posterior of the initial states. In (\textbf{b}) the seemingly correct estimation close to the terminal state still does not prevent some errors from occurring closer to the initial state.}
    \label{fig:longproptest}
\end{figure}

\subsection{\textcolor{red}{Temporary} Why doesn't corridor work?}

Consider the case where the current position in the corridor is the only input and the terminal state is position 5. If the agent is in position 4 it should learn that action right will give one return. In position 3 it should prefer action right, but it does not know if it will reach the end of the corridor in time. The total return will either be 0 or 1. This means the expected value of the action should be 0.5. 

\begin{align}
    \label{eq:reg1}
    G &= X\beta_g + \sigma_g \\
    \label{eq:reg2}
    Q &= X\beta_q + \sigma_q \\
    \label{eq:reg3}
    E[Q] &= X\beta_q \\
    \label{eq:reg4}
    E[G] &= X\beta_q + \sigma_q = X\beta_g
\end{align}

A target consisting of the Monte Carlo total return will result in the regression in equation \ref{eq:reg1}. A target consisting only of Q-values will result in the regression in equation \ref{eq:reg2}. However the actual target $r + Q(s',a')$ is a mix between the two so given a stochastic reward equation \ref{eq:reg2} will over estimate the variance.

However this cannot be the problem in the corridor example since the reward is deterministic so $E[r|x] = R$.

\subsection{\textcolor{red}{Temporary} Stability is (maybe) not a problem, propagation is just slow}

\subsubsection{Sensitivy of the Q-value}

With better priors stability doesn't seem to be a problem. I was using a suggested weak prior from Gelman which was $\alpha=0.001$ and $\beta=0.001$ which leads to inf values in the Scipy library. Essentially But I'm leaving this here cause the reasoning behind this does cause issues. In addition I was using the wrong Invgamma parameterization. This probably caused some instability as well.

\subsubsection{Slow propagation}

A problem is the "speed" of propagation. The issues shown in figure \ref{fig:longproptest} are dealt with by just training for a longer time. However, the amount of training required for good results seems to grow exponentially with the number of states to propagate over.

Keep in mind that the next state which is used as the current states target. I think the slow propagation is due to the wrong target values used to train the bayesian regression before the next states posterior is correct. The further away from the actual environment return, the longer time it takes before the next states posterior is correct which means the model is trained on more bad data.

I think retraining the model from scratch at certain intervals fixes the second part of the issue. To speed up the time it takes for the uncertainty to propagate we can also consider using n-step updates.

\section{Bayesian Deep Q Network}

\todo this section assumes DQN has been explained

The predominant issue with bayesian methods in deep reinforcement learning is using bayesian methods with neural networks. This thesis will address the linear layer method (\todo Actual name for method), a simple and computationally efficient method that comes at the cost of accuracy. 

The final layer in a DQN is a linear layer. Since bayesian regression is also a linear combination one can replace the final layer with a bayesian regression model per action. This is equivalent to rewriting the regression task to 

\begin{equation*}
	Q = \phi(X)\beta + \varepsilon \quad \text{where} \quad \varepsilon \sim N(0,\sigma^2)
\end{equation*}

where $\phi(X)$ is the neural networks output given an input $X$. Note that this means the bayesian regression no longer incorporates all the uncertainty since the above assumes no uncertainty in the $\phi(X)$ encoding. 

Training the model now needs to be split into two processes. Firstly the bayesian regression is trained using the posterior update shown above. The neural network is trained using a similar loss function as the DQN. However the networks Q-value estimate is replaced by the MAP estimate of $\beta$ resulting in

\begin{equation*}
	\theta = \theta - \alpha\nabla_\theta\big(Q_t - [\mu_n^T\phi_\theta(x_t)]\big)^2.
\end{equation*}

Note that these do not have to happen sequentially. In \cite{azziz_2018} and this implementation the bayesian regression is updated less often than the neural network.

Finally to deal with the fact that reinforcement learning is a non-stationary problem the the bayesian regression is trained for scratch each time it is updated.

\cleardoublepage