%===================================== CHAP 1 =================================

\chapter{Introduction}\label{ch:intro}

Reinforcement learning (RL) is a field that has received a large amount of attention in recent years. A RL method tries to learn an optimal sequence of actions based on sparse rewards for it's actions. Advances in the field have resulted in super-human results in many well-known games. RL was the first algorithm to succesfully beat one of the worlds best Go players \cite{silver_2017}. In addition RL methods have learned to fly helicopters\cite{abbeel_2007}, play at a super-human level in Atari games \cite{mnih_2015} and reduce server cluster cooling expenses by 40\% at Google\cite{deepmind}.

What makes RL especially interesting to research is it's generalization potential.  StockFish, the defacto algorithm for playing and analyzing chess, is dependent on hundreds of hard-coded heuristics specific for Chess. \cite{silver_2017b} uses the same method to learn go, chess and shogi, using no hardcoded heuristics. \cite{mnih_2015} learns to play over 40 completely different Atari games using the same exact algorithm.

Despite the recent success of RL there are a large variety of issues that remain. \cite{sutton_barto_2018} has an entire chapter dedicated future issues that need to be solved. A popular blog post that was passed around in the RL community \citep{blog} brought up limitations with RL methods as they are today. 

\cite{bellemare_2012} introduced the ALE, a simple to use package that allows RL methods to play RL games.  Due to it's simplicity and the wide array of different games, it is often used to evaluate new models, for example in \cite{mnih_2015}. This suite of Atari games clearly shows some of these limits in RL. 

One key point in \cite{blog} was that the sample-efficiency of RL is low. For example, \cite{mnih_2015} required that the RL method played each game for 200 million frames. The current state of the art results still require 20 million frames per game \citep{hessel_2017}. Not only does this require a large amount of computation power and memory, it is also simply infeasible for most practical scenarios.

Another issue is that there are certain games in ALE which have proved difficult to solve using RL. One of the hardest to solve is called Montezuma's Revenge(MR). In this game most methods struggle to get out of the first room as there are many ways to lose the game and only a few complicated combinations of actions that lead to good results. This game is viewed as a problem that requires good exploration. This is a concept that will be discussed in depth in the theory section. The underlying idea is that for the RL method to learn this complicated combination of actions is a good idea, it must actually perform. This is unlikely to happen if the agent acts completely randomly so an informed exploration of possible actions is needed.

This project considers the paper \cite{donoghue_2017} which tries to combine statistical uncertainty with current RL methods to minimize the isses above. The paper attempts to increase the level of exploration in areas the method is uncertain about the correct action while decreasing exploration in areas with high certainty. Using this concept, \cite{donoghue_2017} managed to get out of the first room in MR without any heuristics. This was the highest performance achieved in MR when published.

This project will go in detail through the theory behind \cite{donoghue_2017} method. Then to test the method the classical RL environment known as cartpole introduced in \cite{barto_sutton_1983} will be used to compare and analyze the positive and negative sides to using the method described in \cite{donoghue_2017}.

Before moving on to the theory section, it is important to note that RL related problems have not only been researched in the field of computer science. In fields such as control theory and operations research the same problem and approaches are often discussed. Another popular name for reinforcement learning in these fields is approximate dynamic programming and some of the sources used throughout this report refer to the topic under this name. \citep[p.~16]{powell_2011}

\cleardoublepage