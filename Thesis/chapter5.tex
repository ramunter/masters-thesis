\chapter{Discussion}\label{ch:disc}

\section{BNIG model limitations}

\subsection{State Independent Variance}

One constraint of the BNIG model is that it assumes a constant variance over all states given an action. This does not necessarily deteriorate performance as much as one might expect. \cite{azziz_2018} achieves performance better than DQN in multiple ALE games with the same restriction. However it seems that this might be a problem on environments that require more complex exploration. For example Montezuma's revenge is not included in the attempts in \cite{azziz_2018} and attempts at using the BNIG model converges to a zero reward policy.

It is likely that an environment such as Montezuma's Revenge requires the variance to be adjusted per state. In these environments the same action might lead to states with largely different varainces based on the current state. Consider a left action on the edge of one room in Montezuma's Revenge. This leads to a new room with a large set of possibilities and thus should have a large variance. In another case a left action might cause the character to fall to its death. The agent should recognize what is happening and have a low variance estimate that there won't be anymore reward. With the current BNIG model the variance estimate for the left action will have to be between the two resulting in too much exploration in states the model should be certain about and more importantly too little exploration in unexplored states.

In a game like cartpole this is still the case but to much smaller extent. The state of the cartpole has little effect on the variance of the next state. Once a policy is learned the pole is relatively certainly going to stay balanced over future states meaning low variance. If the pole is starting to fall the pole will most likely completly fall leading to termination, once again with low variance. In these types of environments one should expect BNIG to perform well.

\subsection{Variance Stability}

Another shortcoming of the BNIG DQN method developed in this thesis is the instability of the variance estimate. Though an increase in the prior scale parameter improves performance this lowers variance based on a hyperparameter rather than the data. It greatly constricts the exploration and must be tuned on an enviornment to enviornment basis. Further developments should focus on how to stabalize this variance to allow for weaker priors. There are multiple issues that could decrease the stability of the variance.


\subsubsection{Maximization Bias}

One source of instability could be maximization bias\citep[p.~134]{sutton_barto_2018} occuring on the variance estimate. Early in the learning process the maximum operator on the target leads to picking actions that lead to high variance states as these provide the highest values. This leads to higher variance targets which starts a feedback loop that can lead to the large jumps in the rate parameter of the inverse gamma distribution as seen in figure \ref{fig:scale_stability}.

A solution to the maximization bias in regular Q-learning is double Q-learning\citep[p.~134]{sutton_barto_2018}. This consists of creating two completely decoupled models and for each update use one for target action selection and one for action-value calculation. Which is used for what is randomly chosen each update. This doubles the memory complexity of the model but keeps the same computational complexity. The double DQN uses an approximation to this by using the target network for target action selection and the online network for action-value calculation. This is also what is done with the BNIG target model in this thesis. However a complete decoupling by creating two BNIG models per action might help reduce the variance instability.

\subsubsection{State Independent Variance}

It has already been discussed how the state independent variance might be constraining the environments BNIG is usefull on. However it is also possible that this is leading to variance instability on all environments. 

Consider regular Q-learning. The magnitude of the Q-values is controlled by the terminal states where the target is bound by the maximum reward from the environment. As Q-learning learns, this bounded value is propagated backwards towards the initial state, leading to Q-values that are realistic relative to the return from the environment.

In the variance case one should expect the same behaviour. The propagation tests in chapter \ref{ch:linear} showed that given a known posterior state all other states leading up to this will converge towards the same variance. However this test is in a tabular setting where each state has its own variance parameter. In the linear setting a state that should have low variance, such as a penultimate state, will have to have a similar variance to all other states. As such there is no low variance state that slowly bounds the variance of all states. This allows for an ever increasing variance. If one instead has a state dependent variance one could expect a similar result as in Q-learning, with the low variance terminal states slowly bounding the variance of the rest of the states.

\subsubsection{Multimodal Returns}

\todo Is this actually a problem? It doesn't seem to be when I think about it. But I feel that these large changes here indicate we are partially modelling the wrong target. Won't $y^Ty - \beta^T\Lambda\beta$ when $y = r$ push the model towards modelling the return distribution variance? 
 
Another possible source is caused by the terminal state variance estimate. This is an issue when the model believes an episode will keep going but instead something unforseen happens resulting in the episode terminating. In games such as cartpole or acrobot the prediction will be of magnitude $10^2$ while the target will be of magnitude $10^0$. A direct effect of this is large drops in the scale parameter. However this also effects the mean estimate, pushing it to be lower to accomidate for the model taking wrong. This results in a higher target than prediction in all other states which pushes the variance up.

\section{Method Improvements}

\subsection{Allowing for State Dependent Variance}

\subsection{Run Time}

\cleardoublepage