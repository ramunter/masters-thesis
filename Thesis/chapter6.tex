%===================================== CHAP 5 =================================

\chapter{Conclusion}\label{ch:conc}

Motivated by the results from the UBE method \citep{donoghue_2017} this thesis has tried to use bayesian linear regression to drive efficient exploration using Thompson sampling. Through a linear toy example it was shown that previous bayesian linear regression RL methods do not properly propagate variance between states leading to an underestimation of variance. Furthermore it was shown that by sampling action-value targets from the posterior and including the regression noise variance as a bayesian parameter it is possible to achieve proper variance propagation. To the authors knowledge this is the first result in RL showing variance propagation using only conjugate prior regression.

This method was then extended to neural networks based on the similar \cite{azziz_2018} BDQN. The novel method, coined the BNIG DQN, showed promising results on multiple environments but proved to be unstable on some environments. This thesis suggests that further work should focus on making the variance estimate dependent on the environment state. This should both increase the stability of the variance and improve performance on environments that require complex exploration, such as Montezuma's Revenge.

\cleardoublepage